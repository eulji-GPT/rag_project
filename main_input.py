# main.py
from rag_pipeline import build_vector_db, search_docs
from ollama_wrapper import get_ollama_answer

# 1. í¬ë¡œë§ˆDBì— ë¬¸ì„œ ë“±ë¡ (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰í•˜ë©´ ë¨, ì´ë¯¸ í–ˆë‹¤ë©´ ì£¼ì„ì²˜ë¦¬í•´ë„ ë¨)
build_vector_db(data_folder="./data")

print("ğŸ’¬ RAG ì‹œìŠ¤í…œì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ):\n")

try:
    while True:
        query = input("ğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ")

        if query.lower() in ("exit", "quit"):
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 2. ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_chunks = search_docs(query)

        # 3. ê´€ë ¨ ë¬¸ì„œ + ì§ˆë¬¸ â†’ LLM ì‘ë‹µ
        response = get_ollama_answer(query, relevant_chunks, stream=True)  # stream=Trueë„ ê°€ëŠ¥
        print("\nğŸ§  ì‘ë‹µ:", response)
        print("\n" + "-" * 50)

except KeyboardInterrupt:
    print("\nğŸ›‘ ê°•ì œ ì¢…ë£Œë¨.")
