# ollama_wrapper.py
import ollama

# ğŸ”§ RAG í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_rag_prompt(query: str, chunks: list[str], metadatas: list[dict]) -> str:
    prompt = "ë‹¹ì‹ ì€ ì •ë³´ë¥¼ ìš”ì•½í•´ì„œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.\n\n"
    for i, chunk in enumerate(chunks):
        title = metadatas[i].get("title", "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ")
        prompt += f"[ë¬¸ì„œ ì œëª©: {title}]\në‚´ìš©:\n{chunk.strip()}\n\n---\n\n"
    prompt += f"ì§ˆë¬¸: {query}\në‹µë³€:"
    return prompt

# ğŸ”„ Ollama í˜¸ì¶œ í•¨ìˆ˜
def get_ollama_answer(query: str, context_chunks: list[str], metadatas: list[dict], stream: bool = True) -> str:
    prompt = build_rag_prompt(query, context_chunks, metadatas)
    messages = [{"role": "user", "content": prompt}]

    if stream:
        print("\nğŸ’¬ AI ì‘ë‹µ:")
        response = ollama.chat(model="exaone3.5:7.8b", messages=messages, stream=True)
        for chunk in response:
            print(chunk["message"]["content"], end="", flush=True)
        print("\n")
        return ""
    else:
        response = ollama.chat(model="exaone3.5:7.8b", messages=messages)
        return response["message"]["content"]
