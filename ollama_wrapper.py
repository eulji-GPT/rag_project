# ollama_wrapper.py
import ollama

# ë¬¸ì„œë“¤ê³¼ ì§ˆë¬¸ì„ ë¬¶ì–´ LLMì— ì „ë‹¬
def get_ollama_answer(query: str, context_docs: list[str], stream: bool = True) -> str:
    prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{chr(10).join(context_docs)}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""

    messages = [{"role": "user", "content": prompt}]

    if stream:
        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        response = ollama.chat(model="exaone3.5:7.8b", messages=messages, stream=True)
        print("ğŸ’¬ ë‹µë³€:")
        for chunk in response:
            print(chunk["message"]["content"], end="", flush=True)
        return ""
    else:
        # í•œë²ˆì— ì‘ë‹µ
        response = ollama.chat(model="exaone3.5:7.8b", messages=messages)
        return response["message"]["content"]
