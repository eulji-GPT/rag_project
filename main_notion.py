# main.py
from rag_pipeline import search_docs  # build_vector_dbëŠ” ì œê±°
from ollama_wrapper import get_ollama_answer

print("ğŸ’¬ Notion ê¸°ë°˜ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ):\n")

try:
    while True:
        query = input("ğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ")

        if query.lower() in ("exit", "quit"):
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 1. ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ChromaDBì—ì„œ)
        relevant_chunks = search_docs(query)
        
        print("\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:")
        for i, chunk in enumerate(relevant_chunks):
            print(f"[{i+1}] {chunk[:100]}...")  # ì• 100ìë§Œ ë¯¸ë¦¬ë³´ê¸°


        # 2. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ â†’ Ollama ì‘ë‹µ
        response = get_ollama_answer(query, relevant_chunks, stream=True)
        print("\nğŸ§  ì‘ë‹µ:", response)
        print("\n" + "-" * 50)

except KeyboardInterrupt:
    print("\nğŸ›‘ ê°•ì œ ì¢…ë£Œë¨.")
