# main.py
from rag_pipeline import build_vector_db, search_docs
from ollama_wrapper import get_ollama_answer

# 1. í¬ë¡œë§ˆDBì— ë¬¸ì„œ ë“±ë¡ (í•œ ë²ˆë§Œ ì‹¤í–‰)
build_vector_db(data_folder="./test_data")

print("ğŸ’¬ RAG ì‹œìŠ¤í…œì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ):\n")

try:
    while True:
        query = input("ğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ")

        if query.lower() in ("exit", "quit"):
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 2. ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_chunks, metadatas = search_docs(query)

        print("\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:")
        for i, chunk in enumerate(relevant_chunks):
            title = metadatas[i].get("title", "ì œëª© ì—†ìŒ")
            print(f"[{i+1}] ({title}) {chunk[:100]}...")

        # 3. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ â†’ Ollama ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)
        print("\nğŸ’¬ AI ì‘ë‹µ:")
        get_ollama_answer(query, relevant_chunks, metadatas, stream=True)
        print("\n" + "-" * 50)

except KeyboardInterrupt:
    print("\nğŸ›‘ ê°•ì œ ì¢…ë£Œë¨.")
