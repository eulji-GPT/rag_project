# rag_pipeline.py
from sentence_transformers import SentenceTransformer
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

# ì „ì—­ ì„¤ì •
DB_DIR = "./chroma_db"
COLLECTION_NAME = "notion_docs"  # Notionìš© ì»¬ë ‰ì…˜ìœ¼ë¡œ ìˆ˜ì •
EMBED_MODEL = SentenceTransformer("jhgan/ko-sbert-sts")

# ì§ˆì˜ â†’ ì„ë² ë”© â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

def search_docs(query: str, k: int = 3):
    client = chromadb.PersistentClient(path=DB_DIR)
    collection = client.get_or_create_collection(name=COLLECTION_NAME)

    query_embedding = EMBED_MODEL.encode([query]).tolist()
    results = collection.query(query_embeddings=query_embedding, n_results=k)

    return results["documents"][0], results["metadatas"][0]

# ğŸ”§ ë¬¸ì„œ ë“±ë¡ í•¨ìˆ˜
def build_vector_db(data_folder: str):
    client = chromadb.PersistentClient(path=DB_DIR)
    collection = client.get_or_create_collection(name=COLLECTION_NAME)
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)

    # í´ë” ë‚´ ëª¨ë“  .txt íŒŒì¼ ì²˜ë¦¬
    files = [f for f in os.listdir(data_folder) if f.endswith(".txt")]
    if not files:
        print(f"âš ï¸ '{data_folder}' í´ë”ì— .txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file_name in files:
        file_path = os.path.join(data_folder, file_name)
        if not os.path.exists(file_path):
            print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
            continue

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        chunks = splitter.split_text(content)
        for i, chunk in enumerate(chunks):
            chunk_id = f"{file_name}_{i}"
            try:
                collection.add(
                    documents=[chunk],
                    embeddings=EMBED_MODEL.encode([chunk]).tolist(),
                    ids=[chunk_id],
                    metadatas=[{"title": file_name, "chunk_index": i}]
                )
                print(f"âœ… '{file_name}' chunk {i} ì¶”ê°€ë¨")
            except chromadb.errors.IDAlreadyExistsError:
                print(f"âš ï¸ '{file_name}' chunk {i} ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒëµë¨")